---
model: Qwen/Qwen1.5-0.5B-Chat
train_type: lora
dataset: ../data/merged_strategy_dataset.json
torch_dtype: bfloat16
output_dir: ../output/lora/financial_strategy
num_train_epochs: 10
max_length: 2048
lora_rank: 8
lora_alpha: 32
lora_dropout: 0.05
target_modules: all-linear
gradient_checkpointing: true
per_device_train_batch_size: 4
weight_decay: 0.1
learning_rate: 3e-5
gradient_accumulation_steps: 8
max_grad_norm: 0.5
warmup_ratio: 0.03
eval_steps: 100
save_steps: 100
save_total_limit: 2
logging_steps: 10
dataloader_num_workers: 0
...